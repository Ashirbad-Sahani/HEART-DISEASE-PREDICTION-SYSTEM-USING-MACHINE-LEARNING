# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14n9Ih-_q2zWT3aIfpcP_5a1FHbDCwk8O

üìå **Project Title:**

**Early Diagnosis of Heart Disease in India using Predictive Machine Learning Techniques**


 **Step-by-Step Execution Flow:**

1.  **Importing Required Libraries**

   Libraries such as `numpy`, `pandas`, `matplotlib`, `seaborn`, `sklearn`, `xgboost`, and `shap` are imported for data handling, visualization, preprocessing, modeling, and explainability.

2.  **Loading the Dataset**

   ```python
   heart_df = pd.read_csv('/content/heart.csv')
   ```

   The dataset is loaded into a pandas DataFrame.

3.  **Initial Data Exploration**

   * Checking structure (`.info()`)
   * Summary statistics (`.describe()`)
   * Shape, missing values, and data types

4.  **Visualizing Distributions**

   * Distribution plots (`sns.distplot`, `sns.histplot`)
   * Multiple subplots to visualize all features

5.  **Correlation Analysis**

   * Heatmap of correlation matrix to find multicollinearity between features using `sns.heatmap`.

6.  **Feature Transformation**

   * Log transformation is applied to features with high variance:

     ```python
     heart_df['trestbps'] = np.log(heart_df['trestbps'])
     heart_df['chol'] = np.log(heart_df['chol'])
     heart_df['thalach'] = np.log(heart_df['thalach'])
     ```

7.  **Train-Test Split**

   * Splitting the dataset into training and testing sets using `train_test_split`.

8.  **Model Training and Evaluation**

   Multiple classification models are trained and evaluated:

   * **Logistic Regression**
   * **K-Nearest Neighbors (KNN)**
   * **Support Vector Machine (SVM)**
   * **Decision Tree**
   * **Decision Tree with GridSearchCV**
   * **Random Forest**
   * **Gradient Boosting**
   * **XGBoost**

   For each model:

   * Training & testing accuracy
   * Confusion matrix
   * Classification report

9.  **Hyperparameter Tuning**

   * Performed using `GridSearchCV` on the Decision Tree model to improve performance.

10.  **Comparison of Model Performance**

    * A bar chart is plotted to compare accuracy scores of all models.

11.  **Model Serialization**

    * The best model (`RandomForestClassifier`) is saved using `pickle`:

      ```python
      pickle.dump(model, open("/content/heart.pkl",'wb'))
      ```

12.  **ROC Curve Analysis**

    * ROC curves and AUC scores are plotted for all models for visual comparison.

13.  **Combined Performance Plot**

    * Accuracy vs. ROC AUC values are compared using grouped bar charts.

14.  **Feature Importance Visualization**

    * Feature importance from Random Forest is plotted using a horizontal bar plot.

15. **Model Explainability with SHAP**

    * SHAP (SHapley Additive exPlanations) values are computed and visualized using:

      * Force plot
      * Beeswarm plot
      * Summary plot

16.  **User Interface for Predictions**

    * An interactive prediction UI is created using `ipywidgets`:

      * Input widgets for each feature
      * ‚ÄúPredict‚Äù button to classify input as `Heart Disease` or `No Heart Disease` using the saved model.
"""

# Commented out IPython magic to ensure Python compatibility.
# importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

import warnings
warnings.filterwarnings('ignore')

sns.set()
plt.style.use('ggplot')
# %matplotlib inline

#import dataset//

heart_df = pd.read_csv('/content/heart.csv')
heart_df.head(10)
# information about the dataset
heart_df.info()

#description about dataset
heart_df.describe()

heart_df.shape

heart_df.isnull().sum()

heart_df.notnull().sum()

heart_df.dtypes

#Plotting the distribution plot.
plt.figure(figsize=(20,25))
plotnumber=1

for column in heart_df:
    if plotnumber<14:
        ax=plt.subplot(4,4,plotnumber)
        sns.distplot(heart_df[column])
        plt.xlabel(column,fontsize=20)
        plt.ylabel('Values',fontsize=20)
    plotnumber+=1
plt.show()


import seaborn as sns
import matplotlib.pyplot as plt

sns.set_style("whitegrid")  # Clean professional background
plt.figure(figsize=(20, 25))
plotnumber = 1

# Use a consistent professional color (e.g., muted blue)
for column in heart_df:
    if plotnumber < 14:
        ax = plt.subplot(4, 4, plotnumber)
        sns.histplot(heart_df[column], kde=True, color='royalblue')  # modern version of distplot
        plt.xlabel(column, fontsize=18)
        plt.ylabel('Values', fontsize=18)
        plt.title(f'Distribution of {column}', fontsize=20)
        plt.grid(True, linestyle='--', linewidth=0.5)
    plotnumber += 1

plt.tight_layout()
plt.show()

#Correlation matrix

plt.figure(figsize = (16, 8))

corr = heart_df.corr()
mask = np.triu(np.ones_like(corr, dtype = bool))
sns.heatmap(corr, mask = mask, annot = True, fmt = '.2g', linewidths = 1)
plt.show()

# Correlation matrix with updated color
plt.figure(figsize = (16, 8))

corr = heart_df.corr()
mask = np.triu(np.ones_like(corr, dtype = bool))
sns.heatmap(corr, mask=mask, annot=True, fmt='.2g', linewidths=1,
            cmap='coolwarm', center=0, cbar_kws={"shrink": 0.8})

plt.title('Correlation Matrix', fontsize=20)
plt.show()

#checking the variance
heart_df.var()

heart_df['trestbps']=np.log(heart_df['trestbps'])
heart_df['chol']=np.log(heart_df['chol'])
heart_df['thalach']=np.log(heart_df['thalach'])

np.var(heart_df[["trestbps",'chol','thalach']])

heart_df.isnull().sum()

x=heart_df.drop('target',axis=1)
y=heart_df['target']
#x=heart_df.drop('condition',axis=1)
#y=heart_df['condition']

#spliting the dataset
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.30, random_state=0)

x.info()

accuracies={}

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
lr = LogisticRegression(penalty='l2')
lr.fit(x_train,y_train)

y_pred = lr.predict(x_test)

acc=accuracy_score(y_test,y_pred)
accuracies['LR']=acc*100
print("Training accuracy score of the model is:",accuracy_score(y_train, lr.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred))

print("Classification Report",classification_report(y_test,y_pred))

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier(n_neighbors=8)

knn.fit(x_train,y_train)

y_pred1 = knn.predict(x_test)

acc1=accuracy_score(y_test,y_pred1)
accuracies['KNN']=acc1*100

print("Training accuracy score of the model is:",accuracy_score(y_train, knn.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred1)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred1))

print("Classification Report",classification_report(y_test,y_pred1))

from sklearn.svm import SVC

svc = SVC(probability=True)
svc.fit(x_train, y_train)

y_pred2 = svc.predict(x_test)

acc2=accuracy_score(y_test,y_pred2)
accuracies['SVM']=acc2*100

print("Training accuracy score of the model is:",accuracy_score(y_train, svc.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred2)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred2))

print("Classification Report",classification_report(y_test,y_pred2))

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier()
dtc.fit(x_train, y_train)

y_pred3 = dtc.predict(x_test)

acc3=accuracy_score(y_test,y_pred3)
accuracies['DT']=acc3*100

print("Training accuracy score of the model is:",accuracy_score(y_train, dtc.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred3)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred3))

print("Classification Report",classification_report(y_test,y_pred3))

from sklearn.model_selection import GridSearchCV

grid_params = {
    'criterion' : ['gini', 'entropy'],
    'max_depth' : range(2, 32, 1),
    'min_samples_leaf' : range(1, 10, 1),
    'min_samples_split' : range(2, 10, 1),
    'splitter' : ['best', 'random']
}

grid_search = GridSearchCV(dtc, grid_params, cv = 10, n_jobs = -1, verbose = 1)
grid_search.fit(x_train, y_train)

grid_search.best_score_
grid_search.best_params_

dtc2 = DecisionTreeClassifier(criterion= 'entropy', max_depth= 12, min_samples_leaf= 1, min_samples_split= 2, splitter= 'random')
dtc2.fit(x_train, y_train)

y_pred4 = dtc2.predict(x_test)
acc4=accuracy_score(y_test,y_pred4)
accuracies['DT2']=acc4*100

print("Training accuracy score of the model is:",accuracy_score(y_train, dtc2.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred4)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred4))

print("Classification Report",classification_report(y_test,y_pred4))

# update dictionary
accuracies['DT']=acc4*100
del accuracies['DT2']

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(criterion = 'gini', max_depth = 7, max_features = 'sqrt', min_samples_leaf = 2, min_samples_split = 4, n_estimators = 180)
rfc.fit(x_train, y_train)

y_pred5 = rfc.predict(x_test)

acc5=accuracy_score(y_test,y_pred5)
accuracies['RF']=acc5*100

print("Training accuracy score of the model is:",accuracy_score(y_train, rfc.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred5)*100,"%")



print("Confusion matrix of the model",confusion_matrix(y_test,y_pred5))

print("Classification Report",classification_report(y_test,y_pred5))

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()

# Change 'loss' to 'log_loss'
gbc = GradientBoostingClassifier(learning_rate = 0.05, loss = 'log_loss', n_estimators = 180)
gbc.fit(x_train, y_train)

y_pred6 = gbc.predict(x_test)

acc6 = accuracy_score(y_test,y_pred6)
accuracies['GradientBoosting']=acc6*100

print("Training accuracy score of the model is:",accuracy_score(y_train, gbc.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred6)*100,"%")

from xgboost import XGBClassifier

xgb = XGBClassifier(objective = 'binary:logistic', learning_rate = 0.01, max_depth = 5, n_estimators = 180)

xgb.fit(x_train, y_train)

y_pred7 = xgb.predict(x_test)

acc7=accuracy_score(y_test,y_pred7)

accuracies['XGBoost']=acc7*100
print("Training accuracy score of the model is:",accuracy_score(y_train, xgb.predict(x_train))*100,"%")
print("Testing accuracy score of the model is:",accuracy_score(y_test,y_pred7)*100,"%")

print("Confusion matrix of the model",confusion_matrix(y_test,y_pred7))

print("Classification Report",classification_report(y_test,y_pred7))

import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Assuming you already have 'accuracies' dictionary
# Example:
# accuracies = {'SVM': 85, 'RF': 92, 'DT': 78, 'KNN': 88, 'XGB': 95, 'NB': 70}

# Generate a blue gradient palette with as many colors as your algorithms
num_bars = len(accuracies)
blue_palette = sns.color_palette("Blues", num_bars)  # Light to dark blue

plt.figure(figsize=(16,8))
plt.yticks(np.arange(0, 1200, 10))
plt.ylabel("Accuracy %")
plt.xlabel("Algorithms")

sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=blue_palette)
plt.show()

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'KNN', 'SVM',  'Decision Tree', 'Random Forest', 'Gradient Boosting', 'XgBoost'],
    'Score': [acc, acc1, acc2, acc4, acc5, acc6, acc7]
})

models.sort_values(by = 'Score', ascending = False)

import pickle
model = rfc
pickle.dump(model, open("/content/heart.pkl",'wb'))

from sklearn import metrics
plt.figure(figsize=(8,5))
models = [
{
    'label': 'LR',
    'model': lr,
},
{
    'label': 'DT',
    'model': dtc2,
},
{
    'label': 'SVM',
    'model': svc,
},
{
    'label': 'KNN',
    'model': knn,
},
{
    'label': 'XGBoost',
    'model': xgb,
},
{
    'label': 'RF',
    'model': rfc,
},
{
    'label': 'GBDT',
    'model': gbc,
}
]
for m in models:
    model = m['model']
    model.fit(x_train, y_train)
    y_pred=model.predict(x_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(x_test)[:,1])
    auc = metrics.roc_auc_score(y_test,model.predict(x_test))
    plt.plot(fpr1, tpr1, label='%s - ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)
plt.title('ROC - Heart Disease Prediction', fontsize=12)
plt.legend(loc="lower right", fontsize=12)
#plt.savefig("outputs/roc_heart.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()




from sklearn import metrics
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set a clean style
sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))

# Define distinct colors for each model
colors = sns.color_palette("tab10", n_colors=len(models))

# Plot ROC curves
for i, m in enumerate(models):
    model = m['model']
    model.fit(x_train, y_train)
    y_proba = model.predict_proba(x_test)[:, 1]
    fpr, tpr, _ = metrics.roc_curve(y_test, y_proba)
    auc = metrics.roc_auc_score(y_test, model.predict(x_test))
    plt.plot(fpr, tpr, label=f"{m['label']} (AUC = {auc:.2f})", color=colors[i], linewidth=2)

# Plot diagonal line
plt.plot([0, 1], [0, 1], 'k--', lw=1.2, label='Random Classifier')

# Formatting
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=14)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=14)
plt.title('ROC Curve - Heart Disease Prediction', fontsize=16)
plt.legend(loc="lower right", fontsize=11)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()

# Save and show
#plt.savefig("outputs/roc_heart.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt
models = [
{
    'label': 'LR',
    'model': lr,
},
{
    'label': 'DT',
    'model': dtc2,
},
{
    'label': 'SVM',
    'model': svc,
},
{
    'label': 'KNN',
    'model': knn,
},
{
    'label': 'XGBoost',
    'model': xgb,
},
{
    'label': 'RF',
    'model': rfc,
},
{
    'label': 'GBDT',
    'model': gbc,
}
]

means_roc = []
means_accuracy = [100*round(acc,4), 100*round(acc4,4), 100*round(acc2,4), 100*round(acc1,4), 100*round(acc7,4),
                  100*round(acc5,4), 100*round(acc6,4)]

for m in models:
    model = m['model']
    model.fit(x_train, y_train)
    y_pred=model.predict(x_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(x_test)[:,1])
    auc = metrics.roc_auc_score(y_test,model.predict(x_test))
    auc = 100*round(auc,4)
    means_roc.append(auc)

print(means_accuracy)
print(means_roc)

# data to plot
n_groups = 7
means_accuracy = tuple(means_accuracy)
means_roc = tuple(means_roc)

# create plot
fig, ax = plt.subplots(figsize=(8,5))
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.8

rects1 = plt.bar(index, means_accuracy, bar_width,
alpha=opacity,
color='mediumpurple',
label='Accuracy (%)')

rects2 = plt.bar(index + bar_width, means_roc, bar_width,
alpha=opacity,
color='rebeccapurple',
label='ROC (%)')

plt.xlim([-1, 8])
plt.ylim([70, 105])

plt.title('Performance Evaluation - Heart Disease Prediction', fontsize=12)
plt.xticks(index, ('   LR', '   DT', '   SVM', '   KNN', 'XGBoost' , '   RF', '   GBDT'), rotation=40, ha='center', fontsize=12)
plt.legend(loc="upper right", fontsize=10)
#plt.savefig("outputs/PE_heart.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()



import matplotlib.pyplot as plt
import numpy as np

# Data
n_groups = 7
means_accuracy = [100*round(acc,4), 100*round(acc4,4), 100*round(acc2,4), 100*round(acc1,4),
                  100*round(acc7,4), 100*round(acc5,4), 100*round(acc6,4)]
means_roc = []

for m in models:
    model = m['model']
    model.fit(x_train, y_train)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(x_test)[:,1])
    auc = metrics.roc_auc_score(y_test, model.predict(x_test))
    means_roc.append(100*round(auc, 4))

means_accuracy = tuple(means_accuracy)
means_roc = tuple(means_roc)

# Bar positions
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.9

# Create plot
fig, ax = plt.subplots(figsize=(10,6))

# Bar for Accuracy
rects1 = plt.bar(index, means_accuracy, bar_width,
                 alpha=opacity,
                 color='skyblue',
                 edgecolor='navy',
                 hatch='//',
                 label='Accuracy (%)')

# Bar for ROC
rects2 = plt.bar(index + bar_width, means_roc, bar_width,
                 alpha=opacity,
                 color='steelblue',
                 edgecolor='darkblue',
                 hatch='xx',
                 label='ROC (%)')

# Aesthetics
plt.xlim([-0.5, n_groups])
plt.ylim([70, 105])
plt.ylabel('Performance (%)', fontsize=12)
plt.title('Performance Evaluation - Heart Disease Prediction', fontsize=14)
plt.xticks(index + bar_width / 2,
           ('LR', 'DT', 'SVM', 'KNN', 'XGBoost', 'RF', 'GBDT'),
           rotation=30, ha='right', fontsize=11)
plt.legend(loc="upper right", fontsize=10)
plt.grid(axis='y', linestyle='--', linewidth=0.5)

# Save
plt.tight_layout()
#plt.savefig("outputs/PE_heart.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

# Calculate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using Seaborn
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Display feature importances
trained_model = rfc  # Replace rfc with your desired trained model
feature_importances = pd.DataFrame({'Feature': x.columns, 'Importance': trained_model.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Display feature importances with a bar plot using seaborn
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette="Blues_d")  # Use a formal blue palette
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances')
plt.tight_layout()
plt.show()

!pip install shap
import shap
# Before calling shap_values
# Summarize background data using shap.sample
# background_data = shap.sample(x_train, 100)  # Use 100 samples for example

# Or summarize using shap.kmeans
background_data = shap.kmeans(x_train, 10)  # Use 10 clusters for example

# explain the model's predictions using SHAP values
# Using KernelExplainer
explainer = shap.KernelExplainer(model.predict_proba, background_data)
shap_values = explainer.shap_values(x_test)

# Assuming 'model' is your trained RandomForestClassifier (rfc)
y_probs = model.predict_proba(x_test)[:, 1]  # Probabilities of class 1 (heart disease)

patient_number = np.argmin(y_probs)
shap.initjs()
# Access shap values for the specific class (class 1 in this case)
shap.force_plot(explainer.expected_value[1], shap_values[patient_number, :, 1], x_test.iloc[patient_number, :], plot_cmap="RdBu")

#Create a beeswarm plot to visualize the impact of features on predictions.
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(x_test)

# Access the appropriate dimension of SHAP values, for multi-class this should be a list
shap_values_for_class_1 = shap_values  # Instead of shap_values[1]

# If you have a binary classification, ensure you are using the correct index (0 or 1) for your target class
# shap_values_for_class_1 = shap_values[:, 1]  # If it is binary classification, use this instead

shap.summary_plot(shap_values_for_class_1, x_test, show=False)
plt.title("SHAP Summary Plot - Impact on Heart Disease Diagnosis")
plt.xlabel("Impact on Model Output (Proximity to Heart Disease)")
plt.ylabel("Feature")
plt.show()

# Ensure necessary libraries are imported
import pandas as pd
import ipywidgets as widgets
from IPython.display import display
from ipywidgets import Layout, Box
import pickle
import numpy as np # Needed for potential np.log if you applied it to input features

# Dictionary to store the input widgets
input_widgets = {}

# Function to create the input widgets
def create_input_widgets():
    global input_widgets # Declare input_widgets as global if you need to modify it within the function

    # Create widgets for each feature
    input_widgets['age'] = widgets.IntSlider(description='Age:', min=18, max=100, step=1, layout=Layout(width='auto', height='auto'))
    input_widgets['sex'] = widgets.RadioButtons(options=[('Female', 0), ('Male', 1)], description='Sex:', layout=Layout(width='auto', height='auto'))
    # Assuming these were RadioButtons based on the make_prediction logic
    input_widgets['cp_1'] = widgets.RadioButtons(options=[('Atypical Angina', 1), ('Other', 0)], description='CP Type 1:', layout=Layout(width='auto', height='auto'))
    input_widgets['cp_2'] = widgets.RadioButtons(options=[('Non-anginal Pain', 1), ('Other', 0)], description='CP Type 2:', layout=Layout(width='auto', height='auto'))
    input_widgets['cp_3'] = widgets.RadioButtons(options=[('Typical Angina', 1), ('Other', 0)], description='CP Type 3:', layout=Layout(width='auto', height='auto'))
    input_widgets['trestbps'] = widgets.FloatSlider(description='Resting BP:', min=80, max=200, step=1.0, layout=Layout(width='auto', height='auto'))
    input_widgets['chol'] = widgets.FloatSlider(description='Cholesterol:', min=100, max=600, step=1.0, layout=Layout(width='auto', height='auto'))
    input_widgets['fbs'] = widgets.RadioButtons(options=[('> 120 mg/dl', 1), ('<= 120 mg/dl', 0)], description='Fasting Blood Sugar:', layout=Layout(width='auto', height='auto'))
    input_widgets['restecg'] = widgets.RadioButtons(options=[('Normal', 0), ('ST-T Abnormality', 1), ('Left Ventricular Hypertrophy', 2)], description='Resting ECG:', layout=Layout(width='auto', height='auto'))
    input_widgets['thalach'] = widgets.FloatSlider(description='Max Heart Rate:', min=60, max=220, step=1.0, layout=Layout(width='auto', height='auto'))
    input_widgets['exang'] = widgets.RadioButtons(options=[('Yes', 1), ('No', 0)], description='Exercise Induced Angina:', layout=Layout(width='auto', height='auto'))
    input_widgets['oldpeak'] = widgets.FloatSlider(description='ST Depression:', min=0.0, max=6.2, step=0.1, layout=Layout(width='auto', height='auto'))
    input_widgets['slope_1'] = widgets.RadioButtons(options=[('Upsloping', 1), ('Other', 0)], description='ST Slope 1:', layout=Layout(width='auto', height='auto'))
    input_widgets['slope_2'] = widgets.RadioButtons(options=[('Flat', 1), ('Other', 0)], description='ST Slope 2:', layout=Layout(width='auto', height='auto'))
    input_widgets['ca'] = widgets.IntSlider(description='Major Vessels (0-3):', min=0, max=3, step=1, layout=Layout(width='auto', height='auto'))
    # Assuming these were RadioButtons based on the make_prediction logic
    input_widgets['thal_2'] = widgets.RadioButtons(options=[('Fixed Defect', 1), ('Other', 0)], description='Thal 2:', layout=Layout(width='auto', height='auto'))
    input_widgets['thal_3'] = widgets.RadioButtons(options=[('Reversible Defect', 1), ('Other', 0)], description='Thal 3:', layout=Layout(width='auto', height='auto'))


    # Arrange widgets in a box
    input_box = Box(children=list(input_widgets.values()), layout=Layout(
        display='flex',
        flex_flow='column',
        border='solid 1px lightgray',
        padding='10px',
        margin='10px',
        align_items='stretch',
        width='50%'
    ))

    # Display the input box
    display(input_box)


# Load the model
with open("/content/heart.pkl", 'rb') as file:
    model = pickle.load(file)

def make_prediction(b):
    # Get the values from the widgets
    input_data = {name: widget.value for name, widget in input_widgets.items()}

    # Create a dictionary to represent the one-hot encoded row
    one_hot_input = {}

    # Map continuous/binary features directly and apply transformations if necessary
    one_hot_input['age'] = input_data['age']
    one_hot_input['sex'] = input_data['sex']

    # Apply log transformation to features that were transformed during training
    # Add checks for positive values before taking log
    one_hot_input['trestbps'] = np.log(input_data['trestbps']) if input_data['trestbps'] > 0 else 0 # Using 0 as placeholder, handle appropriately if needed
    one_hot_input['chol'] = np.log(input_data['chol']) if input_data['chol'] > 0 else 0 # Using 0 as placeholder, handle appropriately if needed
    one_hot_input['thalach'] = np.log(input_data['thalach']) if input_data['thalach'] > 0 else 0 # Using 0 as placeholder, handle appropriately if needed


    one_hot_input['fbs'] = input_data['fbs']
    one_hot_input['restecg'] = input_data['restecg']

    one_hot_input['exang'] = input_data['exang']
    one_hot_input['oldpeak'] = input_data['oldpeak']
    one_hot_input['ca'] = input_data['ca']


    # Manually create one-hot encoded columns for 'cp', 'thal', 'slope'
    # based on which radio button is selected (value is 1)
    # Assuming 'cp_0', 'thal_1', 'slope_0' were the reference categories (columns dropped by drop_first=True)

    # One-hot encode 'cp'
    one_hot_input['cp_1'] = input_data.get('cp_1', 0) # Use .get to handle potential missing keys if widgets change
    one_hot_input['cp_2'] = input_data.get('cp_2', 0)
    one_hot_input['cp_3'] = input_data.get('cp_3', 0)

    # One-hot encode 'thal'
    one_hot_input['thal_2'] = input_data.get('thal_2', 0)
    one_hot_input['thal_3'] = input_data.get('thal_3', 0)

    # One-hot encode 'slope'
    one_hot_input['slope_1'] = input_data.get('slope_1', 0)
    one_hot_input['slope_2'] = input_data.get('slope_2', 0)

    # Create a DataFrame from the manually created one-hot encoded dictionary
    input_df_processed = pd.DataFrame([one_hot_input])

    # Ensure the columns are in the same order as the training data (x_train)
    # and that all columns present in x_train are also in input_df_processed (adding missing ones with 0)
    # This step is crucial to match the model's expected input format.
    missing_cols = set(x_train.columns) - set(input_df_processed.columns)
    for c in missing_cols:
        input_df_processed[c] = 0

    # Ensure the order of columns matches the training data
    input_df_processed = input_df_processed[x_train.columns]

    # Make prediction
    # Need to handle potential NaN values resulting from log transformation if input was 0 or negative.
    # Depending on how your model handles NaNs, you might need to impute or use a model that handles them.
    # Assuming your model was trained without NaNs after log transformation, you might want to
    # add checks or imputation here if you expect inputs that result in NaNs.
    # For this fix, we assume valid inputs are provided.

    prediction = model.predict(input_df_processed)  # Use the loaded model

    # Display the prediction
    print(f"Prediction: {'Heart Disease' if prediction[0] == 1 else 'No Heart Disease'}")

# Button to make prediction
predict_button = widgets.Button(description="Predict")
predict_button.on_click(make_prediction)

# Function to setup the interface (re-run this cell to update widget creation if you change it)
# (Keep this function as is from the original code if you choose to keep the RadioButtons for cp, thal, slope)
def setup_interface():
    create_input_widgets()
    display(predict_button)

# Call the function to setup the interface
setup_interface()